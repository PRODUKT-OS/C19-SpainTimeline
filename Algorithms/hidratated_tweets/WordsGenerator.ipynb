{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwyeVchYFr6a"
   },
   "source": [
    "# **Required installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install spacy\n",
    "!pip install gensim\n",
    "!pip install ijson\n",
    "!pip install nltk\n",
    "!pip install unidecode\n",
    "!pip install spacy_spanish_lemmatizer\n",
    "!pip install pyLDAvis\n",
    "!pip install stanza\n",
    "!pip install twarc \n",
    "!pip install tweepy \n",
    "!pip install argparse\n",
    "!pip install xtract\n",
    "!pip install wget\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwyeVchYFr6a"
   },
   "source": [
    "# **Generate Twitter API Keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "CONSUMER_KEY = \"\" \n",
    "CONSUMER_SECRET_KEY = \"\"\n",
    "ACCESS_TOKEN_KEY = \"\" \n",
    "ACCESS_TOKEN_SECRET_KEY = \"\" \n",
    "\n",
    "with open('api_keys.json', 'w') as outfile:\n",
    "    json.dump({\n",
    "    \"consumer_key\":CONSUMER_KEY,\n",
    "    \"consumer_secret\":CONSUMER_SECRET_KEY,\n",
    "    \"access_token\":ACCESS_TOKEN_KEY,\n",
    "    \"access_token_secret\": ACCESS_TOKEN_SECRET_KEY\n",
    "     }, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwyeVchYFr6a"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import spacy, gensim\n",
    "import re\n",
    "import string\n",
    "import unidecode\n",
    "import ijson\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import stanza \n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "!python3 -m spacy download es\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwyeVchYFr6a"
   },
   "source": [
    "# **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_YEAR = 1900\n",
    "MAX_YEAR = 2100\n",
    "first_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "second_stopwords = [\"a\",\"via\",\"ultimo\",\"último\",\"solo\",\"actualmente\",\"adelante\",\"además\",\"afirmó\",\"agregó\",\"ahora\",\"ahí\",\"al\",\"algo\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"algún\",\"alrededor\",\"ambos\",\"ampleamos\",\"ante\",\"anterior\",\"antes\",\"apenas\",\"aproximadamente\",\"aquel\",\"aquellas\",\"aquellos\",\"aqui\",\"aquí\",\"arriba\",\"aseguró\",\"así\",\"atras\",\"aunque\",\"ayer\",\"añadió\",\"aún\",\"bajo\",\"bastante\",\"bien\",\"buen\",\"buena\",\"buenas\",\"bueno\",\"buenos\",\"cada\",\"casi\",\"cerca\",\"cierta\",\"ciertas\",\"cierto\",\"ciertos\",\"cinco\",\"comentó\",\"como\",\"con\",\"conocer\",\"conseguimos\",\"conseguir\",\"considera\",\"consideró\",\"consigo\",\"consigue\",\"consiguen\",\"consigues\",\"contra\",\"cosas\",\"creo\",\"cual\",\"cuales\",\"cualquier\",\"cuando\",\"cuanto\",\"cuatro\",\"cuenta\",\"cómo\",\"da\",\"dado\",\"dan\",\"dar\",\"de\",\"debe\",\"deben\",\"debido\",\"decir\",\"dejó\",\"del\",\"demás\",\"dentro\",\"desde\",\"después\",\"dice\",\"dicen\",\"dicho\",\"dieron\",\"diferente\",\"diferentes\",\"dijeron\",\"dijo\",\"dio\",\"donde\",\"dos\",\"durante\",\"e\",\"ejemplo\",\"el\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"embargo\",\"empleais\",\"emplean\",\"emplear\",\"empleas\",\"empleo\",\"en\",\"encima\",\"encuentra\",\"entonces\",\"entre\",\"era\",\"erais\",\"eramos\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estabais\",\"estaban\",\"estabas\",\"estad\",\"estada\",\"estadas\",\"estado\",\"estados\",\"estais\",\"estamos\",\"estan\",\"estando\",\"estar\",\"estaremos\",\"estará\",\"estarán\",\"estarás\",\"estaré\",\"estaréis\",\"estaría\",\"estaríais\",\"estaríamos\",\"estarían\",\"estarías\",\"estas\",\"este\",\"estemos\",\"esto\",\"estos\",\"estoy\",\"estuve\",\"estuviera\",\"estuvierais\",\"estuvieran\",\"estuvieras\",\"estuvieron\",\"estuviese\",\"estuvieseis\",\"estuviesen\",\"estuvieses\",\"estuvimos\",\"estuviste\",\"estuvisteis\",\"estuviéramos\",\"estuviésemos\",\"estuvo\",\"está\",\"estábamos\",\"estáis\",\"están\",\"estás\",\"esté\",\"estéis\",\"estén\",\"estés\",\"ex\",\"existe\",\"existen\",\"explicó\",\"expresó\",\"fin\",\"fue\",\"fuera\",\"fuerais\",\"fueran\",\"fueras\",\"fueron\",\"fuese\",\"fueseis\",\"fuesen\",\"fueses\",\"fui\",\"fuimos\",\"fuiste\",\"fuisteis\",\"fuéramos\",\"fuésemos\",\"gran\",\"grandes\",\"gueno\",\"ha\",\"haber\",\"habida\",\"habidas\",\"habido\",\"habidos\",\"habiendo\",\"habremos\",\"habrá\",\"habrán\",\"habrás\",\"habré\",\"habréis\",\"habría\",\"habríais\",\"habríamos\",\"habrían\",\"habrías\",\"habéis\",\"había\",\"habíais\",\"habíamos\",\"habían\",\"habías\",\"hace\",\"haceis\",\"hacemos\",\"hacen\",\"hacer\",\"hacerlo\",\"haces\",\"hacia\",\"haciendo\",\"hago\",\"han\",\"has\",\"hasta\",\"hay\",\"haya\",\"hayamos\",\"hayan\",\"hayas\",\"hayáis\",\"he\",\"hecho\",\"hemos\",\"hicieron\",\"hizo\",\"hoy\",\"hube\",\"hubiera\",\"hubierais\",\"hubieran\",\"hubieras\",\"hubieron\",\"hubiese\",\"hubieseis\",\"hubiesen\",\"hubieses\",\"hubimos\",\"hubiste\",\"hubisteis\",\"hubiéramos\",\"hubiésemos\",\"hubo\",\"igual\",\"incluso\",\"indicó\",\"informó\",\"intenta\",\"intentais\",\"intentamos\",\"intentan\",\"intentar\",\"intentas\",\"intento\",\"ir\",\"junto\",\"la\",\"lado\",\"largo\",\"las\",\"le\",\"les\",\"llegó\",\"lleva\",\"llevar\",\"lo\",\"los\",\"luego\",\"lugar\",\"manera\",\"manifestó\",\"mayor\",\"me\",\"mediante\",\"mejor\",\"mencionó\",\"menos\",\"mi\",\"mientras\",\"mio\",\"mis\",\"misma\",\"mismas\",\"mismo\",\"mismos\",\"modo\",\"momento\",\"mucha\",\"muchas\",\"mucho\",\"muchos\",\"muy\",\"más\",\"mí\",\"mía\",\"mías\",\"mío\",\"míos\",\"nada\",\"nadie\",\"ni\",\"ninguna\",\"ningunas\",\"ninguno\",\"ningunos\",\"ningún\",\"no\",\"nos\",\"nosotras\",\"nosotros\",\"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"nueva\",\"nuevas\",\"nuevo\",\"nuevos\",\"nunca\",\"o\",\"ocho\",\"os\",\"otra\",\"otras\",\"otro\",\"otros\",\"para\",\"parece\",\"parte\",\"partir\",\"pasada\",\"pasado\",\"pero\",\"pesar\",\"poca\",\"pocas\",\"poco\",\"pocos\",\"podeis\",\"podemos\",\"poder\",\"podria\",\"podriais\",\"podriamos\",\"podrian\",\"podrias\",\"podrá\",\"podrán\",\"podría\",\"podrían\",\"poner\",\"por\",\"porqué\",\"porque\",\"posible\",\"primer\",\"primera\",\"primero\",\"primeros\",\"principalmente\",\"propia\",\"propias\",\"propio\",\"propios\",\"próximo\",\"próximos\",\"pudo\",\"pueda\",\"puede\",\"pueden\",\"puedo\",\"pues\",\"que\",\"quedó\",\"queremos\",\"quien\",\"quienes\",\"quiere\",\"quién\",\"qué\",\"realizado\",\"realizar\",\"realizó\",\"respecto\",\"sabe\",\"sabeis\",\"sabemos\",\"saben\",\"saber\",\"sabes\",\"se\",\"sea\",\"seamos\",\"sean\",\"seas\",\"segunda\",\"segundo\",\"según\",\"seis\",\"ser\",\"seremos\",\"será\",\"serán\",\"serás\",\"seré\",\"seréis\",\"sería\",\"seríais\",\"seríamos\",\"serían\",\"serías\",\"seáis\",\"señaló\",\"si\",\"sido\",\"siempre\",\"siendo\",\"siete\",\"sigue\",\"siguiente\",\"sin\",\"sino\",\"sobre\",\"sois\",\"sola\",\"solamente\",\"solas\",\"solo\",\"solos\",\"somos\",\"son\",\"soy\",\"su\",\"sus\",\"suya\",\"suyas\",\"suyo\",\"suyos\",\"sí\",\"sólo\",\"tal\",\"también\",\"tampoco\",\"tan\",\"tanto\",\"te\",\"tendremos\",\"tendrá\",\"tendrán\",\"tendrás\",\"tendré\",\"tendréis\",\"tendría\",\"tendríais\",\"tendríamos\",\"tendrían\",\"tendrías\",\"tened\",\"teneis\",\"tenemos\",\"tener\",\"tenga\",\"tengamos\",\"tengan\",\"tengas\",\"tengo\",\"tengáis\",\"tenida\",\"tenidas\",\"tenido\",\"tenidos\",\"teniendo\",\"tenéis\",\"tenía\",\"teníais\",\"teníamos\",\"tenían\",\"tenías\",\"tercera\",\"ti\",\"tiempo\",\"tiene\",\"tienen\",\"tienes\",\"toda\",\"todas\",\"todavía\",\"todo\",\"todos\",\"total\",\"trabaja\",\"trabajais\",\"trabajamos\",\"trabajan\",\"trabajar\",\"trabajas\",\"trabajo\",\"tras\",\"trata\",\"través\",\"tres\",\"tu\",\"tus\",\"tuve\",\"tuviera\",\"tuvierais\",\"tuvieran\",\"tuvieras\",\"tuvieron\",\"tuviese\",\"tuvieseis\",\"tuviesen\",\"tuvieses\",\"tuvimos\",\"tuviste\",\"tuvisteis\",\"tuviéramos\",\"tuviésemos\",\"tuvo\",\"tuya\",\"tuyas\",\"tuyo\",\"tuyos\",\"tú\",\"ultimo\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usa\",\"usais\",\"usamos\",\"usan\",\"usar\",\"usas\",\"uso\",\"usted\",\"va\",\"vais\",\"valor\",\"vamos\",\"van\",\"varias\",\"varios\",\"vaya\",\"veces\",\"ver\",\"verdad\",\"verdadera\",\"verdadero\",\"vez\",\"vosotras\",\"vosotros\",\"voy\",\"vuestra\",\"vuestras\",\"vuestro\",\"vuestros\",\"y\",\"ya\",\"yo\",\"él\",\"éramos\",\"ésta\",\"éstas\",\"éste\",\"éstos\",\"última\",\"últimas\",\"último\",\"últimos\"]\n",
    "mswa = [unidecode.unidecode(w) for w in my_stopwords]\n",
    "sswa = [unidecode.unidecode(w) for w in second_stopwords]\n",
    "my_stopwords = first_stopwords+second_stopwords+mswa+sswa\n",
    "word_rooter = nltk.stem.SnowballStemmer('spanish').stem\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "my_punctuation = '!¡\"$%&\\'()*+,-./:;<=>¿?[\\\\]^_`{|}~•@.\"\"-,`'\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "search_params = {'n_components': [5], 'learning_decay': [.5, .7, .9]}\n",
    "stNLP = stanza.Pipeline(processors='tokenize,mwt,pos,lemma', lang='es', use_gpu=True,tokenize_pretokenized=True) \n",
    "vectorizer = CountVectorizer(strip_accents='ascii',max_df=0.9, min_df=250, stop_words=my_stopwords, lowercase=True, token_pattern='[a-zA-Z0-9]{3,}')\n",
    "\n",
    "def get_url_patern():\n",
    "    return re.compile(\n",
    "        r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))'\n",
    "        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "\n",
    "\n",
    "def get_emojis_pattern():\n",
    "    try:\n",
    "        # UCS-4\n",
    "        emojis_pattern = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n",
    "    except re.error:\n",
    "        # UCS-2\n",
    "        emojis_pattern = re.compile(\n",
    "            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')\n",
    "    return emojis_pattern\n",
    "\n",
    "\n",
    "def get_hashtags_pattern():\n",
    "    return re.compile(r'#\\w*')\n",
    "\n",
    "\n",
    "def get_single_letter_words_pattern():\n",
    "    return re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])')\n",
    "\n",
    "\n",
    "def get_blank_spaces_pattern():\n",
    "    return re.compile(r'\\s{2,}|\\t')\n",
    "\n",
    "\n",
    "def get_twitter_reserved_words_pattern():\n",
    "    return re.compile(r'(RT |rt | RT| rt| RT | rt |FAV|fav|VIA|via)')\n",
    "\n",
    "\n",
    "def get_mentions_pattern():\n",
    "    return re.compile(r'@\\w*')\n",
    "\n",
    "def is_year(text):\n",
    "    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR < len(text) < MAX_YEAR):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "class TwitterPreprocessor:\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "\n",
    "    def fully_preprocess(self):\n",
    "        return self \\\n",
    "            .remove_urls() \\\n",
    "            .remove_mentions() \\\n",
    "            .remove_hashtags() \\\n",
    "            .remove_twitter_reserved_words() \\\n",
    "            .remove_punctuation() \\\n",
    "            .remove_single_letter_words() \\\n",
    "            .remove_blank_spaces() \\\n",
    "            .remove_stopwords() \\\n",
    "            .remove_numbers() \\\n",
    "            .remove_accents()\n",
    "    \n",
    "    def remove_accents(self):\n",
    "        self.text = unidecode.unidecode(self.text)\n",
    "        return self\n",
    "    def remove_urls(self):\n",
    "        self.text = re.sub(pattern=get_url_patern(), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        self.text = self.text.translate(str.maketrans('', '', my_punctuation))\n",
    "        return self\n",
    "\n",
    "    def remove_mentions(self):\n",
    "        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_hashtags(self):\n",
    "        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_twitter_reserved_words(self):\n",
    "        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_single_letter_words(self):\n",
    "        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_blank_spaces(self):\n",
    "        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_stopwords(self, extra_stopwords=None):\n",
    "        if extra_stopwords is None:\n",
    "            extra_stopwords = []\n",
    "        text = nltk.word_tokenize(self.text)\n",
    "        stop_words = set(my_stopwords)\n",
    "\n",
    "        new_sentence = []\n",
    "        for w in text:\n",
    "            if w not in stop_words and w not in extra_stopwords:\n",
    "                new_sentence.append(w)\n",
    "        self.text = ' '.join(new_sentence)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self, preserve_years=False):\n",
    "        text_list = self.text.split(' ')\n",
    "        for text in text_list:\n",
    "            if text.isnumeric():\n",
    "                if preserve_years:\n",
    "                    if not is_year(text):\n",
    "                        text_list.remove(text)\n",
    "                else:\n",
    "                    text_list.remove(text)\n",
    "\n",
    "        self.text = ' '.join(text_list)\n",
    "        return self\n",
    "\n",
    "    def lowercase(self):\n",
    "        self.text = self.text.lower()\n",
    "        return self\n",
    "    \n",
    "    def handle_negations(self):  \n",
    "        self.text = re.sub(pattern=get_negations_pattern(), repl='', string=self.text)\n",
    "        return self\n",
    "\n",
    "def clean_tweet(tweet, bigrams=True):\n",
    "    tweet = tweet.lower()\n",
    "    tw = TwitterPreprocessor(tweet)\n",
    "    tw.fully_preprocess()\n",
    "    return tw.text\n",
    "\n",
    "def lemmatization(tweet, allowed_postags=['NOUN', 'ADJ', 'ADV']):#, 'VERB'\n",
    "    global counterApply\n",
    "    texts_out = []\n",
    "    for sent in tweet:\n",
    "        doc = stNLP(\" \".join(sent)) \n",
    "        for sent in doc.sentences:\n",
    "            for token in sent.words:\n",
    "                if token.pos in allowed_postags:\n",
    "                    if token.lemma not in ['-PRON-']:\n",
    "                        if token.lemma == 'coronaviru':\n",
    "                            texts_out.append(token.lemma+'s')\n",
    "                        else:\n",
    "                            texts_out.append(token.lemma)\n",
    "    counterApply += 1\n",
    "    print(\"Current Row: \" + str(counterApply))\n",
    "    clear_output()\n",
    "    return texts_out\n",
    "\n",
    "def tweet_to_words(tweet):\n",
    "    yield(gensim.utils.simple_preprocess(str(tweet), deacc=True))  \n",
    "\n",
    "def remove_brackets(tweet):\n",
    "    return \",\".join([str(elem) for elem in tweet])\n",
    "\n",
    "def writeJsonTopics(iteration,tweetsJson,filename):\n",
    "    try:\n",
    "        with open('pyLDAvisExport.json') as f:\n",
    "            tb = json.load(f)\n",
    "            res = '['\n",
    "            for i in range(len(tb[\"token.table\"][\"Topic\"])):\n",
    "                res += '{' + '\"Topic\":'+str(tb[\"token.table\"][\"Topic\"][i]) + ','  + '\"Freq\":'+ str(tb[\"token.table\"][\"Freq\"][i]) + ',' + '\"Term\":\"'+ tb[\"token.table\"][\"Term\"][i] + '\"}'\n",
    "                if i != len(tb[\"token.table\"][\"Topic\"])-1:\n",
    "                    res+=\",\"\n",
    "            res += ']'\n",
    "            y = json.loads(res)\n",
    "            t1arr = '['\n",
    "            t2arr = '['\n",
    "            t3arr = '['\n",
    "            t4arr = '['\n",
    "            t5arr = '['\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                output = '{' + '\"text\":\"'+ y[i][\"Term\"] + '\",'  + '\"weight\":'+ str(y[i][\"Freq\"])+ '}'\n",
    "                if(y[i][\"Topic\"] == 1):\n",
    "                    t1arr+=output\n",
    "                    t1arr+=','\n",
    "                if(y[i][\"Topic\"] == 2):\n",
    "                    t2arr+=output\n",
    "                    t2arr+=','\n",
    "                if(y[i][\"Topic\"] == 3):\n",
    "                    t3arr+=output\n",
    "                    t3arr+=','\n",
    "                if(y[i][\"Topic\"] == 4):\n",
    "                    t4arr+=output\n",
    "                    t4arr+=','\n",
    "                if(y[i][\"Topic\"] == 5):\n",
    "                    t5arr+=output\n",
    "                    t5arr+=','\n",
    "            t1arr = t1arr[:-1]+']'\n",
    "            t2arr = t2arr[:-1]+']'\n",
    "            t3arr = t3arr[:-1]+']'\n",
    "            t4arr = t4arr[:-1]+']'\n",
    "            t5arr = t5arr[:-1]+']'\n",
    "            final = '{' + '\"A\":'+t1arr +',\"B\":'+t2arr+',\"C\":'+t3arr+',\"D\":'+t4arr+',\"E\":'+t5arr + \",\" + tweetsJson + '}' \n",
    "            final = \"//Extracted from \" + filename + \"\\n\"+'event_week_'+str(iteration)+\" = \" + final + \"\\n\"\n",
    "            with open(\"words.js\", 'a+') as f_output:\n",
    "                f_output.write(final)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "\n",
    "def getRelevantTweetsJson(best_lda_model,df):\n",
    "    lda_output = best_lda_model.transform(tf)\n",
    "    topicnames = [i for i in range(best_lda_model.n_components)]\n",
    "    docnames = [i for i in range(len(df))]\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['dominant_topic'] = dominant_topic\n",
    "    topicLetters =[\"A\",\"B\",\"C\",\"D\",\"E\"] \n",
    "    rest = ''\n",
    "    for i in range(5):\n",
    "        relevantTweets = df_document_topic[df_document_topic[i] >= 0.8].sort_values(by=[i]).head(3).index.values\n",
    "        rest += '\"T'+ topicLetters[i]+'\":['\n",
    "        if relevantTweets != []:\n",
    "            for j,t in enumerate(relevantTweets):\n",
    "                text = re.sub(pattern=re.compile(r'\\s{2,}|\\t|\\n'), repl=' ', string=df['text'].iloc[[t]].item())\n",
    "                text = unidecode.unidecode(text)\n",
    "                rest += '{' + '\"text\":`'+ text + '`'+ '}'\n",
    "                if j != len(relevantTweets)-1:\n",
    "                        rest+=\",\"\n",
    "            rest+=\"]\"\n",
    "            if i != 4:\n",
    "                rest+=\",\"\n",
    "        else:\n",
    "            rest += ']'\n",
    "            if i != 4:\n",
    "                rest+=\",\"\n",
    "        \n",
    "    return rest\n",
    "\n",
    "def GenerateWords():\n",
    "    files = os.listdir('../tweets/')\n",
    "    files.sort()\n",
    "    if 'GenerateTweetsIds.py' in files:\n",
    "        files.remove('GenerateTweetsIds.py')\n",
    "    directory = './'\n",
    "\n",
    "    for i,filename in enumerate(files):\n",
    "        if os.path.exists(directory+'hydrated_tweets_short.json'):\n",
    "            os.remove(directory+'hydrated_tweets_short.json')\n",
    "        if os.path.exists(directory+'hydrated_tweets.csv'):\n",
    "            os.remove(directory+'hydrated_tweets.csv')\n",
    "        if os.path.exists(directory+'hydrated_tweets.zip'):\n",
    "            os.remove(directory+'hydrated_tweets.zip')\n",
    "        if os.path.exists(directory+'hydrated_tweets'):\n",
    "            os.remove(directory+'hydrated_tweets')\n",
    "        if os.path.exists(directory+'pyLDAvisExport.json'):\n",
    "                    os.remove(directory+'pyLDAvisExport.json')\n",
    "        print(filename)\n",
    "        exeRes = subprocess.run([\"python3\", \"get_metadata.py\", \"-i\", \"../tweets/\" + filename, \"-o\", \"hydrated_tweets\", \"-k\", \"api_keys.json\", \"-m\", \"e\"],capture_output=True)\n",
    "        print(exeRes)\n",
    "        if os.path.exists(directory+'hydrated_tweets_short.json'):\n",
    "            if os.stat('hydrated_tweets_short.json').st_size != 0:\n",
    "                counterApply = 0\n",
    "                dfaux = pd.read_json(os.path.join(directory, \"hydrated_tweets_short.json\"), lines=True)\n",
    "                df = dfaux[['created_at','text']]\n",
    "                df2 = df\n",
    "                df2['clean_tweet'] = df2.text.apply(clean_tweet)\n",
    "                df3 = df2\n",
    "                df3['tokenized_tweet'] = df3.clean_tweet.apply(tweet_to_words)\n",
    "                df4 = df3\n",
    "                df4['lemmatized_tweet'] = df4.tokenized_tweet.apply(lemmatization)\n",
    "                df5 = df4\n",
    "                df5['new_lemmatized_tweet'] = df5.lemmatized_tweet.apply(remove_brackets)\n",
    "                tf = vectorizer.fit_transform(df5['new_lemmatized_tweet']) \n",
    "                tf_feature_names = vectorizer.get_feature_names()\n",
    "                lda = LatentDirichletAllocation()\n",
    "                grid_model = GridSearchCV(lda, param_grid=search_params)\n",
    "                grid_model.fit(tf)\n",
    "                best_lda_model = grid_model.best_estimator_\n",
    "                pyLDAvis.enable_notebook()\n",
    "                panel = pyLDAvis.sklearn.prepare(best_lda_model, tf, vectorizer, mds='tsne')\n",
    "                pyLDAvis.save_json(panel,'pyLDAvisExport.json')\n",
    "                relTweets = getRelevantTweetsJson(best_lda_model,df5)\n",
    "                writeJsonTopics(i+1,relTweets,filename)\n",
    "        return True\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwyeVchYFr6a"
   },
   "source": [
    "# **Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenerateWords()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hydrating_tweets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
